{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "With much of the code written, it's time to think about downloading and storing the data.\n",
    "\n",
    "First of all, let's get a decent estimate of how large an individual GTFS-R feed is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../auth/mta-credentials.json\", \"r\") as f:\n",
    "    key = json.loads(f.read())['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loop 1, sleeping\n",
      "Finished loop 2, sleeping\n",
      "Finished loop 3, sleeping\n",
      "Finished loop 4, sleeping\n",
      "Finished loop 5, sleeping\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "requests_1 = []\n",
    "requests_16 = []\n",
    "requests_21 = []\n",
    "requests_2 = []\n",
    "requests_11 = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    requests_1.append(requests.get(\"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=1\".format(key)))\n",
    "    requests_16.append(requests.get(\"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=16\".format(key)))\n",
    "    requests_21.append(requests.get(\"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=21\".format(key)))\n",
    "    requests_2.append(requests.get(\"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=2\".format(key)))\n",
    "    requests_11.append(requests.get(\"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=11\".format(key)))\n",
    "    print(\"Finished loop {0}, sleeping\".format(i + 1))\n",
    "    if i != 4:\n",
    "        time.sleep(30)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(requests_16[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(requests_1[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the first (and beefiest) feed is currently throwing a permission denied error. Puzzling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Permission denied'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests_1[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017\n",
      "57.897\n",
      "20.534\n",
      "19.977\n",
      "3.401\n"
     ]
    }
   ],
   "source": [
    "for feeds in [requests_1, requests_16, requests_21, requests_2, requests_11]:\n",
    "    print(sum([len(feed.content) for feed in feeds]) / len(requests_1)  / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can still ballpark that feed at around 204 KB in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src/\")\n",
    "from processing import fetch_archival_gtfs_realtime_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.765"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fetch_archival_gtfs_realtime_data(timestamp='2014-09-17-09-31', raw=True)) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829331199999999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(204.765+57.897+20.534+19.977+3.401)*2880/1000/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's say that we are looking at 200 KB, 60 KB, 20 KB, 20 KB, and 3.5 KB responses per feed.\n",
    "\n",
    "Feeds update every 30 seconds, which means that for full resolution we will need to capture 2880 messages per day. That means that our daily storage requirements are:\n",
    "\n",
    "* Feed 1 (1, 2, 3, 4, 5, 6, S): ~200 KB per response, ~0.6 GB per day.\n",
    "* Feed 16 (N, Q, R, W): ~60 KB per response, ~0.17 GB per day.\n",
    "* Feed 21 (B, D), ~20 KB per response, ~0.06 GB per day.\n",
    "* Feed 2 (L), ~20 KB per response, ~0.06 GB per day.\n",
    "* Feed 11 (SIR), ~3.5 KB per response, ~0.01 GB per day.\n",
    "* TOTAL (1, 2, 3, 4, 5, 6, S, N, Q, R, W, B, D, L, SIR), ~0.9 GB per day.\n",
    "\n",
    "Note that these are just estimates, conducted using data from a sample of 8 PM realtimes. As the amount of trains on the track differs quite heavily by time, they're not wholey accurate. But, good ballparks.\n",
    "\n",
    "0.9 GB per day means 28 GB in one month, 82 GB in a season (three months), and 328 GB in a year. For context, my home PC contains 1 TB (1000 GB) of persistent (hard-drive) storage, and 16 GB of random-access memory.\n",
    "\n",
    "These data volumes are managable, but they're beyond anything offered by a free-ish data storage service. For example, Amazon S3 provides 5 GB of storage, 20000 GET requests, and 2000 PUT requests per month in its (12-month limited) free tier. That, particularly the PUT request limit, is nowhere near enough...\n",
    "\n",
    "Let's see how much each set of data would cost us on a few different services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.544"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "328 * 0.023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5256000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2880*5*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.256"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.005 * (1051200 / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1024000000000003"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.004 * (5256000 / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.86"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7.50 + 5.26 + 2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892800"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "31*2880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feeds = [\n",
    "    \"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=1\".format(key),\n",
    "    \"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=21\".format(key),\n",
    "    \"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=2\".format(key),\n",
    "    \"http://datamine.mta.info/mta_esi.php?key={0}&feed_id=11\".format(key)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 11.62 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 3: 486 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit [requests.get(feed) for feed in feeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303.5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200+60+20+20+3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178560"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2880*31*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon S3+Lambda\n",
    "\n",
    "### S3\n",
    "\n",
    "S3 charges a bunch of different prices.\n",
    "\n",
    "First of all, the cost for data storage is 0.023\\$ per GB. That means a year's worth of data would run you a grand total of...7.50\\$.\n",
    "\n",
    "PUT requests (what actually stashes the data into the S3 server) cost 0.005\\$ per 1000 requests. Pushing data into S3 for a year would require `2880*5*365=5256000` PUT requests, costing `0.005 * (1051200 / 1000)` or 5.26$.\n",
    "\n",
    "Finally, GET requests are 0.004\\$ per 10,000 requests. Those same 5256000 objects would cost 2.10$.\n",
    "\n",
    "That means that keeping this data running for a period of a year would cost 14.86\\$. That, obviously, is no problem at all.\n",
    "\n",
    "### Lambda\n",
    "\n",
    "In order to get this data into S3, I would need to run a job every thirty seconds on AWS Lambda.\n",
    "\n",
    "AWS Lambda charges based on the number of requests made, the time it takes to process each request, and how much memory is allocated to that process during the time that it is running.\n",
    "\n",
    "The data itself takes up about `200+60+20+20+3.5 KB`, which is 303.5 KB, which is less than a MB. Including Python and `requests`, and `boto3`, it seems unlikely that we will even fill up the 128 MB tier (the lowest one).\n",
    "\n",
    "OK, so we will use a 128 MB allocation. From the loop above, it seems likely that the entire download-and-stash process will run inside of 1 seconds. Conservatively, let's say it takes 2 seconds to run. That's 178560 seconds of processing time.\n",
    "\n",
    "The AWS Lambda free tier, which is persistant beyond the 12-month trial, gives 3,200,000 free seconds at that 128 MB process level. That's an order of magnitude more than we need!\n",
    "\n",
    "Additionally, you get 1 million free requests per month, many orders of magnitude more than we need.\n",
    "\n",
    "So Lambda is effectively free.\n",
    "\n",
    "Which means the total cost of everything all together comes out to 14.86\\$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
